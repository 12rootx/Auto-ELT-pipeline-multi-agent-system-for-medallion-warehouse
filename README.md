# ğŸ§  Auto ELT Agent

> **Automated ELT pipeline generator** that creates complete data warehouses from **natural language descriptions** using **LangGraph for multi-agent system**, **dlt for ingestion**, and **dbt for transformation**.

---

## ğŸš€ Overview

Describe your desired data pipeline in plain English â€” and let the **Auto ELT Agent** handle the rest.  
It automatically:

- ğŸ” **Analyzes** your data sources (local files or Azure Blob Storage)
- ğŸ—ï¸ **Designs** a complete **Medallion architecture** (Bronze â†’ Silver â†’ Gold â†’ Views)
- ğŸ§± **Generates** `dbt` models for all layers (silver, gold, views, snapshots)
- âš™ï¸ **Sets up** ingestion pipelines with `dlt`
- â„ï¸ **Deploys** to **Snowflake**, with proper incremental/full-refresh strategies
- ğŸ§‘â€ğŸ’» **Includes human-in-the-loop approvals** for key design decisions

---

## ğŸ—ï¸ Architecture

**Medallion Data Flow:**
```
Bronze (Raw) â†’ Silver (Cleaned) â†’ Gold (Business-Ready) â†’ Views (Data Marts)
```

### Generated Components

| Layer | Description |
|--------|-------------|
| **Bronze** | Raw data ingestion via `dlt` |
| **Silver** | Cleaned and standardized models (deduplication, normalization) |
| **Gold** | Fact and dimension tables (star schema) |
| **Snapshots** | SCD Type 2 tracking for historical data |
| **Views** | Aggregated, business-friendly data marts |

---

## ğŸ“ Project Structure

```
elt_pipeline/
â”œâ”€â”€ main.py                        # Entry point
â”œâ”€â”€ graph/                         # LangGraph workflow definition
â”‚   â”œâ”€â”€ graph.py                   # Main workflow graph
â”‚   â”œâ”€â”€ state.py                   # AgentState model
â”‚   â””â”€â”€ routers.py                 # Conditional routing logic
â”œâ”€â”€ agents/                        # Agent implementations
â”‚   â”œâ”€â”€ interpreter.py             # Parse user instructions
â”‚   â”œâ”€â”€ source_inspector.py        # Analyze data sources
â”‚   â”œâ”€â”€ architecture_designer.py   # Design warehouse blueprint
â”‚   â”œâ”€â”€ silver_builder.py          # Generate Silver layer models
â”‚   â”œâ”€â”€ gold_builder.py            # Generate Gold layer models
â”‚   â”œâ”€â”€ view_builder.py            # Create data marts and views
â”‚   â”œâ”€â”€ reviewers.py               # Validation and quality review
â”‚   â””â”€â”€ nodes/                     # Specialized workflow nodes
â”‚       â”œâ”€â”€ human_node.py
â”‚       â”œâ”€â”€ ELT_config_node.py
â”‚       â”œâ”€â”€ design_reuse_checker.py
â”‚       â”œâ”€â”€ dlt_ingestor.py
â”‚       â””â”€â”€ dbt_transformer.py
â”œâ”€â”€ utils/                         # Helper utilities
â”‚   â”œâ”€â”€ helper_functions.py
â”‚   â”œâ”€â”€ dbt_configs.py
â”‚   â”œâ”€â”€ ELTConfigurator.py
â”‚   â”œâ”€â”€ agent_tools.py
â”‚   â””â”€â”€ save_sql.py
â”œâ”€â”€ configs.py                     # Model and directory configurations
â”œâ”€â”€ design_plans/                  # Auto-generated architecture designs
â””â”€â”€ dbt_project/                   # Auto-generated dbt project
```

---

## âš™ï¸ Quick Start

### 1. Create Environment

```bash
conda create -n elt_agent python=3.11 -y
conda activate elt_agent
```

### 2. Install Dependencies

```bash
# Install from requirements
pip install -r requirements.txt

# Or install manually
pip install langgraph langchain-openai dlt pyyaml python-dotenv dbt-core snowflake-connector-python
```

### 3. Configure Environment Variables

Create a `.env` file in the root directory:

```bash
OPENAI_API_KEY=sk-your-openai-key-here

# Optional: Snowflake credentials
SNOWFLAKE_ACCOUNT=your-account
SNOWFLAKE_USER=your-username
SNOWFLAKE_PASSWORD=your-password
SNOWFLAKE_DATABASE=your-database
SNOWFLAKE_WAREHOUSE=your-warehouse
SNOWFLAKE_ROLE=your-role
```

### 4. Configure DLT Secrets

In `.dlt/secrets.toml`, include:

```toml
[destination.snowflake.credentials]
database = "your_database"
username = "your_username"
password = "your_password"
host = "your_account.snowflakecomputing.com"
warehouse = "your_warehouse"
role = "your_role"

[sources.filesystem.credentials]
type = "azure"
container = "your_container_name"
connection_string = "your_connection_string"
```

### 5. Run the System

```bash
python main.py
```

---

## ğŸ’¬ Example Usage

**Example:**

```python
Please ingest the following datasets into Snowflake and build the ELT pipeline: 

Tables: 
  census_lga_2016_g01: /Users/rootx/DataScience_Std/LANGGRAPH/de_agent/datasets/Census LGA/2016Census_G01_NSW_LGA.csv
  census_lga_2016_g02: /Users/rootx/DataScience_Std/LANGGRAPH/de_agent/datasets/Census LGA/2016Census_G02_NSW_LGA.csv
  nsw_lga_code: /Users/rootx/DataScience_Std/LANGGRAPH/de_agent/datasets/NSW_LGA/NSW_LGA_CODE.csv
  nsw_lga_suburb: /Users/rootx/DataScience_Std/LANGGRAPH/de_agent/datasets/NSW_LGA/NSW_LGA_SUBURB.csv
  listings: /Users/rootx/DataScience_Std/LANGGRAPH/de_agent/datasets/listings/*.csv
```

### What Happens Next

1. ğŸ” Inspects all datasets and infers schema  
2. ğŸ—ï¸ Designs the **Medallion architecture** automatically  
3. ğŸ§± Generates dbt models for Bronze, Silver, Gold, and Views  
4. ğŸ‘¥ Requests human approval for schema and transformation logic  
5. âš¡ Sets up incremental/full-refresh strategies  
6. ğŸš€ Generates a fully deployable dbt project

---

## ğŸ§© Example Generated dbt Project

```
dbt_project/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sources.yml
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ slv_customers.sql
â”‚   â”‚   â””â”€â”€ slv_orders.sql
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”œâ”€â”€ fact_orders.sql
â”‚   â”‚   â””â”€â”€ dim_customers.sql
â”‚   â””â”€â”€ views/
â”‚       â””â”€â”€ vw_customer_orders.sql
â”œâ”€â”€ snapshots/
â”‚   â””â”€â”€ snp_customers.sql
â””â”€â”€ dbt_project.yml
```

---

## ğŸ”§ Customization

### Modify Agent Prompts

Edit `agents/prompts.py` to customize:
- Agent behavior
- Output formats
- Design heuristics

### Configure LLMs

Update LLM settings in `configs.py`:

```python
# Example configurations
llm = ChatOpenAI(model="gpt-4o", temperature=0)
llm_en = ChatOpenAI(model="gpt-4.1", temperature=0)  # Enhanced version
```

---

## ğŸ§  Key Features Recap

âœ… Natural language â†’ Complete ELT pipeline  
âœ… Automated architecture generation  
âœ… Human approval checkpoints  
âœ… Seamless integration: dlt + dbt + Snowflake  
âœ… Modular agent-based design  
âœ… Reusable configuration and design plans  

---

**Author:** *Anna Chen*  
**Tech Stack:** LangGraph Â· LangChain Â· dlt Â· dbt Â· Snowflake Â· Python 3.11
