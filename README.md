# ğŸ§  Auto ELT Pipeline Agentic System

> **Automated ELT pipeline generator** that creates complete data warehouses from **natural language descriptions** using **LangGraph for multi-agent system**, **dlt for ingestion**, and **dbt for transformation**.

---

## ğŸš€ Overview

Describe your desired data pipeline in plain English â€” and let the **Auto ELT Agent** handle the rest.  
It automatically:

- ğŸ” **Analyzes** your data sources (local files or Azure Blob Storage)
- ğŸ—ï¸ **Designs** a complete **Medallion architecture** (Bronze â†’ Silver â†’ Gold â†’ Views)
- ğŸ§± **Generates** `dbt` models for all layers (silver, gold, views, snapshots), and reusable, adujstable design plan object.
- âš™ï¸ **Sets up** ingestion pipelines with `dlt`
- â„ï¸ **Deploys** to **Snowflake**, with proper incremental/full-refresh strategies
- ğŸ§‘â€ğŸ’» **Includes human-in-the-loop approvals** for key design decisions

---

## ğŸ—ï¸ Architecture

**Medallion Data Flow:**
```
Bronze (Raw) â†’ Silver (Cleaned) â†’ Gold (Business-Ready) â†’ Views (Data Marts)
```

### Generated Components

| Layer | Description |
|--------|-------------|
| **Bronze** | Raw data ingestion via `dlt` |
| **Silver** | Cleaned and standardized models (deduplication, normalization) |
| **Snapshots** | SCD Type 2 tracking for historical data |
| **Gold** | Fact and dimension tables (star schema) |
| **Views** | Aggregated, business-friendly data marts |

---

## ğŸ“ Project Structure


```
system_dir/
â”œâ”€â”€ main.py                        # Execution point
â”œâ”€â”€ graph/                  
â”‚   â”œâ”€â”€ graph.py                   # Main LangGraph workflow graph
â”‚   â”œâ”€â”€ state.py                   # AgentState model
â”‚   â””â”€â”€ routers.py                 # Conditional routing logic
â”œâ”€â”€ agents/                        
â”‚   â”œâ”€â”€ interpreter.py             # Parse user prompt
â”‚   â”œâ”€â”€ source_inspector.py        # Analyze data sources
â”‚   â”œâ”€â”€ architecture_designer.py   # Design warehouse blueprint
â”‚   â”œâ”€â”€ silver_builder.py          # Generate Silver layer models
â”‚   â”œâ”€â”€ gold_builder.py            # Generate Gold layer models
â”‚   â”œâ”€â”€ view_builder.py            # Create data marts and views
â”‚   â”œâ”€â”€ reviewers.py               # Validation and quality review
â”‚   â””â”€â”€ nodes/                     # Specialized workflow nodes
â”‚       â”œâ”€â”€ human_node.py
â”‚       â”œâ”€â”€ ELT_config_node.py
â”‚       â”œâ”€â”€ design_reuse_checker.py
â”‚       â”œâ”€â”€ dlt_ingestor.py
â”‚       â””â”€â”€ dbt_transformer.py
â”œâ”€â”€ utils/                         # Helper utilities
â”‚   â”œâ”€â”€ helper_functions.py
â”‚   â”œâ”€â”€ dbt_configs.py
â”‚   â”œâ”€â”€ ELTConfigurator.py
â”‚   â”œâ”€â”€ agent_tools.py
â”‚   â””â”€â”€ save_sql.py
â”œâ”€â”€ configs.py                     # Model and output directory configurations
â”œâ”€â”€ design_plans/                  # Auto-generated dbt models after human confirmation on view_builder, for future adjustment/debugging
â””â”€â”€ dbt_project/                   # Auto-generated dbt project from dbt_transformer
```

---

## ğŸ“ Agentis System Workflow
<img width="853" height="590" alt="image" src="https://github.com/user-attachments/assets/4f2583dd-e70e-4dfb-819e-349bae0ceac3" />

---

## âš™ï¸ Quick Start

### 1. Create Environment

```bash
conda create -n elt_agent python=3.11 -y
conda activate elt_agent
```

### 2. Install Dependencies

```bash
# Install from requirements
pip install -r requirements.txt
```

### 3. Configurations
#### 3.1 Environment Variables

Create a `.env` file in the root directory:

```bash
OPENAI_API_KEY=sk-your-openai-key-here

# Destination: Snowflake credentials
SNOWFLAKE_ACCOUNT=your-account
SNOWFLAKE_USER=your-username
SNOWFLAKE_PASSWORD=your-password
SNOWFLAKE_DATABASE=your-database
SNOWFLAKE_WAREHOUSE=your-warehouse
SNOWFLAKE_ROLE=your-role
```

#### 3.2. Configure DLT Secrets

In `.dlt/secrets.toml`, include:

```toml
[destination.snowflake.credentials]
database = "your_database"
username = "your_username"
password = "your_password"
host = "your_host"
warehouse = "your_warehouse"
role = "your_role"

[sources.filesystem.credentials]
type = "azure"
container = "your_container_name"
connection_string = "your_connection_string"
```

#### Reference:
1. dlt hub docs: https://dlthub.com/docs/intro
2. dbt docs: https://docs.getdbt.com/docs/introduction

### 5. Run the System

```bash
cd your_system_dir
```

```bash
python main.py
```

---

## ğŸ’¬ Example Usage

**Example:**

```python
Please ingest the following datasets into Snowflake and build the ELT pipeline: 

Tables: 
  census_lga_2016_g01: ./datasets/Census LGA/2016Census_G01_NSW_LGA.csv
  census_lga_2016_g02: ./datasets/Census LGA/2016Census_G02_NSW_LGA.csv
  nsw_lga_code: ./datasets/NSW_LGA/NSW_LGA_CODE.csv
  nsw_lga_suburb: ./datasets/NSW_LGA/NSW_LGA_SUBURB.csv
  listings: ./datasets/listings/*.csv
```

### What Happens Next

1. ğŸ” Inspects all datasets and infers schema  
2. ğŸ—ï¸ Designs the **Medallion architecture** automatically  
3. ğŸ§± Generates dbt models for Bronze, Silver, Gold, and Views, along with reusable, adujstable design plan object.
4. ğŸ‘¥ Requests human approval for schema and transformation logic  
5. âš¡ Sets up incremental/full-refresh strategies  
6. ğŸš€ Generates a fully dbt project 

---

## ğŸ§© Example Generated dbt Project

```
dbt_project/
â”œâ”€â”€ macros/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sources.yml
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ slv_customers.sql
â”‚   â”‚   â””â”€â”€ slv_orders.sql
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”œâ”€â”€ fact_orders.sql
â”‚   â”‚   â””â”€â”€ dm_customers.sql
â”‚   â””â”€â”€ views/
â”‚       â””â”€â”€ vw_customer_orders.sql
â”œâ”€â”€ snapshots/
â”‚   â””â”€â”€ snp_customers.sql
â”‚â”€â”€ packages.yml
â”‚â”€â”€ profiles.yml
â””â”€â”€ dbt_project.yml
```
#### Check dbt lineage graph

```bash
cd ./dbt_project
```

```bash
dbt docs serve
```
---

## ğŸ”§ Customization

### Modify Agent Prompts

Edit `agents/prompts.py` to customize:
- Agent behavior
- Output formats
- Design heuristics

### Configure LLMs and output_dir

Update LLM settings and output_dir in `configs.py`:

```python
# Current configurations for optimal output

# Basic level for agents: intepreter, source_inspector, dataset_explorer, pipeline_orchestrator
llm = ChatOpenAI(model="gpt-4o", temperature=0)

# Med level for agents: all model builders
llm_md = ChatOpenAI(model="gpt-4.1", temperature=0)

# Enhanced level for agents: architecture_designer, reviewer
llm_en = ChatOpenAI(model="gpt-5", reasoning_effort="low", temperature=0)
```

---

## ğŸ§  Key Features Recap

âœ… Natural language â†’ Complete ELT pipeline  
âœ… Automated architecture generation  
âœ… Human approval checkpoints  
âœ… Seamless integration: dlt + dbt + Snowflake  
âœ… Modular agent-based design  
âœ… Reusable configuration and design plans  

---

**Author:** *Anna Chen*  
**Tech Stack:** LangGraph Â· LangChain Â· dlt Â· dbt Â· Snowflake Â· Python 3.11
